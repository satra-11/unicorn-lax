import { openDB, type DBSchema, type IDBPDatabase } from 'idb'
import type { Photo, ProcessingSession, FaceCluster } from './types'

interface AppDB extends DBSchema {
  photos: {
    key: string // photo id
    value: Photo
    indexes: { 'by-session': string; 'by-timestamp': number; 'by-hash': string }
  }
  sessions: {
    key: string // session id
    value: ProcessingSession
  }
  clusters: {
    key: string // cluster id
    value: FaceCluster
  }
}

const DB_NAME = 'photo-selector-db'
const DB_VERSION = 2

let dbPromise: Promise<IDBPDatabase<AppDB>>

export function getDB() {
  if (!dbPromise) {
    dbPromise = openDB<AppDB>(DB_NAME, DB_VERSION, {
      upgrade(db, oldVersion, newVersion, transaction) {
        if (!db.objectStoreNames.contains('photos')) {
          const store = db.createObjectStore('photos', { keyPath: 'id' })
          store.createIndex('by-session', 'sessionId') // Make sure Photo has sessionId
          store.createIndex('by-timestamp', 'timestamp')
          store.createIndex('by-hash', 'hash')
        } else {
          // Migration for existing DB
          const store = transaction.objectStore('photos')
          if (!store.indexNames.contains('by-hash')) {
            store.createIndex('by-hash', 'hash')
          }
        }
        if (!db.objectStoreNames.contains('sessions')) {
          db.createObjectStore('sessions', { keyPath: 'id' })
        }
        if (!db.objectStoreNames.contains('clusters')) {
          db.createObjectStore('clusters', { keyPath: 'id' })
        }
      },
    })
  }
  return dbPromise
}

export async function savePhoto(photo: Photo) {
  const db = await getDB()
  return db.put('photos', photo)
}

export async function getPhotosBySession(sessionId: string) {
  const db = await getDB()
  // We need to add sessionId to Photo interface in types.ts first if we want to index by it.
  // Assuming we filter manually or add index.
  // For now, let's just get all and filter or use cursor.
  // Actually, let's update Photo interface later or now.
  // But wait, getAllFromIndex is better.
  // We will assume Photo has a sessionId field.
  return db.getAllFromIndex('photos', 'by-session', sessionId)
}

export async function getPhotoByHash(hash: string): Promise<Photo | undefined> {
  const db = await getDB()
  return db.getFromIndex('photos', 'by-hash', hash)
}

export async function getPhoto(id: string) {
  const db = await getDB()
  return db.get('photos', id)
}

export async function saveSession(session: ProcessingSession) {
  const db = await getDB()
  return db.put('sessions', session)
}

export async function getSession(id: string) {
  const db = await getDB()
  return db.get('sessions', id)
}

export async function saveCluster(cluster: FaceCluster) {
  const db = await getDB()
  return db.put('clusters', cluster)
}

export async function getAllClusters(): Promise<FaceCluster[]> {
  const db = await getDB()
  return db.getAll('clusters')
}

export async function deleteCluster(id: string): Promise<void> {
  const db = await getDB()
  await db.delete('clusters', id)
}

export async function updateClusterLabel(id: string, label: string) {
  const db = await getDB()
  const cluster = await db.get('clusters', id)
  if (cluster) {
    cluster.label = label
    return db.put('clusters', cluster)
  }
}

export async function clearExistingData() {
  console.log('Clearing existing data...')
  const db = await getDB()
  await db.clear('photos')
  await db.clear('sessions')
  await db.clear('clusters')
  console.log('Existing data cleared.')
}

export async function clearPhotos() {
  console.log('Clearing photos only...')
  const db = await getDB()

  // 1. Clear Photo and Session stores
  await db.clear('photos')
  await db.clear('sessions')

  // 2. Handle clusters:
  //    - DELETE auto-generated clusters (default "Person X" labels)
  //      to prevent stale clusters from competing with trained ones on next upload
  //    - KEEP user-trained clusters (custom labels) but reset their photo lists
  const clusters = await db.getAll('clusters')
  const tx = db.transaction('clusters', 'readwrite')
  const store = tx.objectStore('clusters')

  const isAutoGenerated = (label: string) => /^(Person|人物) \d+$/.test(label)

  await Promise.all(
    clusters.map((cluster) => {
      if (isAutoGenerated(cluster.label)) {
        return store.delete(cluster.id)
      }
      // Reset photo lists for user-trained clusters
      cluster.photoIds = []
      cluster.confirmedPhotoIds = []
      return store.put(cluster)
    }),
  )

  await tx.done
  console.log('Photos, sessions cleared. Auto-generated clusters deleted, trained clusters reset.')
}

export async function getLastSession(): Promise<ProcessingSession | undefined> {
  const db = await getDB()
  const sessions = await db.getAll('sessions')
  if (sessions.length === 0) return undefined
  return sessions.reduce((latest, current) => {
    return latest.updatedAt > current.updatedAt ? latest : current
  })
}

// --- Backup & Restore Helpers ---

const blobToBase64 = (blob: Blob): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader()
    reader.onloadend = () => {
      const result = reader.result as string
      // Ensure we have a data URL
      if (result.startsWith('data:')) {
        resolve(result)
      } else {
        reject(new Error('Failed to convert blob to base64 data URL'))
      }
    }
    reader.onerror = reject
    reader.readAsDataURL(blob)
  })
}

const base64ToBlob = async (dataUrl: string): Promise<Blob> => {
  // Use fetch if available as it is usually fastest for Data URLs, but fallback or manual parse if needed.
  // Manual parse is robust for ensuring we get the type right if fetch behaves oddly.
  try {
    const arr = dataUrl.split(',')
    if (arr.length < 2) throw new Error('Invalid data URL')

    const mimeMatch = (arr[0] as string).match(/:(.*?);/)
    const mime = mimeMatch ? mimeMatch[1] : 'application/octet-stream'

    const bstr = atob(arr[1] || '')
    let n = bstr.length
    const u8arr = new Uint8Array(n)
    while (n--) {
      u8arr[n] = bstr.charCodeAt(n)
    }
    return new Blob([u8arr], { type: mime })
  } catch (e) {
    console.warn('Manual base64 conversion failed, trying fetch', e)
    const res = await fetch(dataUrl)
    return res.blob()
  }
}

export async function exportDatabase() {
  const db = await getDB()
  const photos = await db.getAll('photos')
  const sessions = await db.getAll('sessions')
  const clusters = await db.getAll('clusters')

  // Serialize Photos: Convert Blobs to Base64
  const serializedPhotos = await Promise.all(
    photos.map(async (p) => {
      let thumbnailBase64 = undefined
      if (p.thumbnail) {
        thumbnailBase64 = await blobToBase64(p.thumbnail)
      }

      // Serialize face thumbnails if they exist
      let faces = undefined
      if (p.faces) {
        faces = await Promise.all(
          p.faces.map(async (f) => {
            let fThumb = undefined
            if (f.thumbnail) {
              fThumb = await blobToBase64(f.thumbnail)
            }
            return {
              ...f,
              descriptor: Array.from(f.descriptor),
              thumbnail: fThumb,
            }
          }),
        )
      }

      return {
        ...p,
        thumbnail: thumbnailBase64,
        faces: faces,
      }
    }),
  )

  // Serialize Clusters: Convert TypedArrays to regular arrays for JSON
  const serializedClusters = await Promise.all(
    clusters.map(async (c) => {
      let thumbnailBase64 = undefined
      if (c.thumbnail) {
        thumbnailBase64 = await blobToBase64(c.thumbnail)
      }
      return {
        ...c,
        descriptor: Array.from(c.descriptor), // Float32Array -> Array
        thumbnail: thumbnailBase64,
      }
    }),
  )

  const data = {
    version: 1,
    timestamp: Date.now(),
    photos: serializedPhotos,
    sessions,
    clusters: serializedClusters,
  }

  return JSON.stringify(data)
}

export async function importDatabase(jsonString: string) {
  try {
    const data = JSON.parse(jsonString)
    if (!data.version || !data.photos || !data.sessions || !data.clusters) {
      throw new Error('Invalid backup file format')
    }

    await clearExistingData()
    const db = await getDB()

    // Restore Sessions
    for (const session of data.sessions) {
      await db.put('sessions', session)
    }

    // Restore Photos
    for (const p of data.photos) {
      let thumbnailBlob = undefined
      if (typeof p.thumbnail === 'string') {
        thumbnailBlob = await base64ToBlob(p.thumbnail)
      }

      let faces = undefined
      if (Array.isArray(p.faces)) {
        faces = await Promise.all(
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          p.faces.map(async (f: any) => {
            let fThumb = undefined
            if (typeof f.thumbnail === 'string') {
              fThumb = await base64ToBlob(f.thumbnail)
            }
            return {
              ...f,
              descriptor: f.descriptor ? new Float32Array(f.descriptor) : undefined,
              thumbnail: fThumb,
            }
          }),
        )
      }

      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      const restoredPhoto: any = {
        ...p,
        thumbnail: thumbnailBlob,
        faces: faces,
      }

      await db.put('photos', restoredPhoto)
    }

    // Restore Clusters
    for (const c of data.clusters) {
      let thumbnailBlob = undefined
      if (typeof c.thumbnail === 'string') {
        thumbnailBlob = await base64ToBlob(c.thumbnail)
      }

      const restoredCluster = {
        ...c,
        descriptor: new Float32Array(c.descriptor),
        thumbnail: thumbnailBlob,
      }
      await db.put('clusters', restoredCluster)
    }

    console.log('Database imported successfully')
    return true
  } catch (e) {
    console.error('Import failed:', e)
    throw e
  }
}
